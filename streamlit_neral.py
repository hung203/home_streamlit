import datetime
import random
from matplotlib import patches
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st
import mlflow
import mlflow.pytorch
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from PIL import Image

# TiÃªu Ä‘á» á»©ng dá»¥ng
st.title("PhÃ¢n loáº¡i chá»¯ sá»‘ viáº¿t tay MNIST vá»›i Neural_Netwwork")

# Táº¡o cÃ¡c tab
tab1, tab2, tab3 = st.tabs([
    "LÃ½ thuyáº¿t",
    "Huáº¥n luyá»‡n",
    "MLflow"
])

# Tab 1: LÃ½ thuyáº¿t
with tab1:
    st.header("HÆ°á»›ng dáº«n: LÃ½ thuyáº¿t tá»•ng quÃ¡t vá» máº¡ng nÆ¡-ron")
    st.markdown("""
    Máº¡ng nÆ¡-ron nhÃ¢n táº¡o (Artificial Neural Networks - ANN) lÃ  má»™t mÃ´ hÃ¬nh há»c mÃ¡y Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a nÃ£o bá»™ con ngÆ°á»i. NÃ³ Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ há»c há»i vÃ  dá»± Ä‘oÃ¡n tá»« dá»¯ liá»‡u thÃ´ng qua cÃ¡c lá»›p nÆ¡-ron káº¿t ná»‘i vá»›i nhau. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c khÃ¡i niá»‡m vÃ  bÆ°á»›c hoáº¡t Ä‘á»™ng tá»•ng quÃ¡t:
    """)

    # Pháº§n 1: Cáº¥u trÃºc cÆ¡ báº£n
    st.markdown("""
    ### 1. Cáº¥u trÃºc cÆ¡ báº£n
    - **NÆ¡-ron (Neuron)**: ÄÆ¡n vá»‹ tÃ­nh toÃ¡n cÆ¡ báº£n, nháº­n Ä‘áº§u vÃ o, xá»­ lÃ½, vÃ  táº¡o Ä‘áº§u ra.
    - **Lá»›p (Layers)**:
      - **Lá»›p Ä‘áº§u vÃ o (Input Layer)**: Nháº­n dá»¯ liá»‡u thÃ´ (vÃ­ dá»¥: hÃ¬nh áº£nh, sá»‘ liá»‡u).
      - **Lá»›p áº©n (Hidden Layers)**: Xá»­ lÃ½ dá»¯ liá»‡u Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng phá»©c táº¡p.
      - **Lá»›p Ä‘áº§u ra (Output Layer)**: ÄÆ°a ra káº¿t quáº£ cuá»‘i cÃ¹ng (vÃ­ dá»¥: phÃ¢n loáº¡i, dá»± Ä‘oÃ¡n sá»‘).
    - **Trá»ng sá»‘ (Weights)** vÃ  **Bias**: CÃ¡c tham sá»‘ Ä‘iá»u chá»‰nh má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a Ä‘áº§u vÃ o, Ä‘Æ°á»£c cáº­p nháº­t trong quÃ¡ trÃ¬nh há»c.
    """)
    st.image("https://miro.medium.com/max/1200/1*FYiM8SggQTVQz_Hrmz6fOw.png", 
             caption="Cáº¥u trÃºc cÆ¡ báº£n cá»§a máº¡ng nÆ¡-ron: Lá»›p Ä‘áº§u vÃ o, lá»›p áº©n, vÃ  lá»›p Ä‘áº§u ra.", width=300)

    # Pháº§n 2: CÃ¡ch hoáº¡t Ä‘á»™ng (chi tiáº¿t tá»«ng bÆ°á»›c vá»›i áº£nh má»›i)
    st.markdown("""
    ### 2. CÃ¡ch hoáº¡t Ä‘á»™ng
    Máº¡ng nÆ¡-ron hoáº¡t Ä‘á»™ng thÃ´ng qua má»™t chuá»—i cÃ¡c bÆ°á»›c tuáº§n tá»±, tá»« viá»‡c nháº­n dá»¯ liá»‡u, xá»­ lÃ½, dá»± Ä‘oÃ¡n, Ä‘áº¿n Ä‘iá»u chá»‰nh Ä‘á»ƒ cáº£i thiá»‡n. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c bÆ°á»›c nguyÃªn lÃ½ hoáº¡t Ä‘á»™ng chi tiáº¿t:
    """)

    st.markdown("""
    #### BÆ°á»›c 1: Nháº­n vÃ  truyá»n dá»¯ liá»‡u Ä‘áº§u vÃ o
    - Dá»¯ liá»‡u thÃ´ (vÃ­ dá»¥: hÃ¬nh áº£nh, sá»‘ liá»‡u) Ä‘Æ°á»£c Ä‘Æ°a vÃ o lá»›p Ä‘áº§u vÃ o.
    - Má»—i nÆ¡-ron trong lá»›p Ä‘áº§u vÃ o Ä‘áº¡i diá»‡n cho má»™t giÃ¡ trá»‹ cá»§a dá»¯ liá»‡u (vÃ­ dá»¥: má»™t pixel trong áº£nh).
    - Dá»¯ liá»‡u sau Ä‘Ã³ Ä‘Æ°á»£c truyá»n Ä‘áº¿n lá»›p áº©n Ä‘áº§u tiÃªn thÃ´ng qua cÃ¡c káº¿t ná»‘i cÃ³ trá»ng sá»‘.
    """)
    st.image("https://i.imgur.com/8g6zK9U.png", 
             caption="Dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Æ°á»£c Ä‘Æ°a vÃ o lá»›p Ä‘áº§u tiÃªn.", width=350)

    st.markdown("""
    #### BÆ°á»›c 2: TÃ­nh tá»•ng trá»ng sá»‘ táº¡i nÆ¡-ron
    - Táº¡i má»—i nÆ¡-ron trong lá»›p áº©n, dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Æ°á»£c nhÃ¢n vá»›i trá»ng sá»‘ tÆ°Æ¡ng á»©ng vÃ  cá»™ng vá»›i bias:
      $$ z = W \\cdot X + b $$
      - \(W\): Ma tráº­n trá»ng sá»‘ (weights).
      - \(X\): Vector dá»¯ liá»‡u Ä‘áº§u vÃ o (inputs).
      - \(b\): GiÃ¡ trá»‹ bias (Ä‘iá»u chá»‰nh).
    - \(z\) lÃ  tá»•ng trá»ng sá»‘, Ä‘áº¡i diá»‡n cho giÃ¡ trá»‹ chÆ°a qua xá»­ lÃ½ cá»§a nÆ¡-ron.
    """)
    st.image("https://i.imgur.com/5p5gXZm.png", 
             caption="TÃ­nh tá»•ng trá»ng sá»‘ táº¡i má»™t nÆ¡-ron.", width=350)

    st.markdown("""
    #### BÆ°á»›c 3: Ãp dá»¥ng hÃ m kÃ­ch hoáº¡t
    - Tá»•ng trá»ng sá»‘ \(z\) Ä‘Æ°á»£c truyá»n qua má»™t hÃ m kÃ­ch hoáº¡t (activation function) Ä‘á»ƒ táº¡o tÃ­nh phi tuyáº¿n:
      - **ReLU**: \( a = \\max(0, z) \) (chá»‰ giá»¯ giÃ¡ trá»‹ dÆ°Æ¡ng).
      - **Sigmoid**: \( a = \\frac{1}{1 + e^{-z}} \) (giá»›i háº¡n Ä‘áº§u ra tá»« 0 Ä‘áº¿n 1).
      - **Tanh**: \( a = \\tanh(z) \) (giá»›i háº¡n Ä‘áº§u ra tá»« -1 Ä‘áº¿n 1).
    - Äáº§u ra \(a\) cá»§a hÃ m kÃ­ch hoáº¡t lÃ  giÃ¡ trá»‹ cuá»‘i cÃ¹ng cá»§a nÆ¡-ron, Ä‘Æ°á»£c truyá»n sang lá»›p tiáº¿p theo.
    """)
    st.image("https://miro.medium.com/max/1200/1*XxxiA0jJvPrHEJHD4z893g.png", 
             caption="Ãp dá»¥ng hÃ m kÃ­ch hoáº¡t (ReLU, Sigmoid, Tanh).", width=400)

    st.markdown("""
    #### BÆ°á»›c 4: Lan truyá»n qua cÃ¡c lá»›p
    - Äáº§u ra cá»§a lá»›p trÆ°á»›c (sau khi qua hÃ m kÃ­ch hoáº¡t) trá»Ÿ thÃ nh Ä‘áº§u vÃ o cá»§a lá»›p tiáº¿p theo.
    - QuÃ¡ trÃ¬nh tÃ­nh tá»•ng trá»ng sá»‘ vÃ  Ã¡p dá»¥ng hÃ m kÃ­ch hoáº¡t láº·p láº¡i qua táº¥t cáº£ cÃ¡c lá»›p áº©n, Ä‘áº¿n lá»›p Ä‘áº§u ra.
    - Lá»›p Ä‘áº§u ra táº¡o ra dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng cá»§a mÃ´ hÃ¬nh (vÃ­ dá»¥: xÃ¡c suáº¥t phÃ¢n loáº¡i).
    """)
    st.image("https://i.imgur.com/Z4N3g5M.png", 
             caption="Lan truyá»n qua cÃ¡c lá»›p tá»« Ä‘áº§u vÃ o Ä‘áº¿n Ä‘áº§u ra.", width=400)

    st.markdown("""
    #### BÆ°á»›c 5: TÃ­nh hÃ m máº¥t mÃ¡t
    - So sÃ¡nh dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vá»›i giÃ¡ trá»‹ thá»±c táº¿ Ä‘á»ƒ Ä‘o sai sá»‘ (loss).
    - VÃ­ dá»¥ hÃ m máº¥t mÃ¡t:
      - **Mean Squared Error (MSE)**: \( L = \\frac{1}{n} \\sum (y - \\hat{y})^2 \) (cho há»“i quy).
      - **Cross-Entropy Loss**: \( L = -\\frac{1}{n} \\sum [y \\cdot \\log(\\hat{y})] \) (cho phÃ¢n loáº¡i).
    - \(y\): GiÃ¡ trá»‹ thá»±c táº¿, \(\\hat{y}\): Dá»± Ä‘oÃ¡n.
    """)
    st.image("https://i.imgur.com/3XzJ4gq.png", 
             caption="TÃ­nh hÃ m máº¥t mÃ¡t Ä‘á»ƒ Ä‘o sai sá»‘.", width=350)

    st.markdown("""
    #### BÆ°á»›c 6: TÃ­nh gradient báº±ng lan truyá»n ngÆ°á»£c
    - DÃ¹ng quy táº¯c chuá»—i (chain rule) Ä‘á»ƒ tÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t theo tá»«ng trá»ng sá»‘ vÃ  bias:
      $$ \\frac{\\partial L}{\\partial W}, \\frac{\\partial L}{\\partial b} $$
    - Gradient chá»‰ ra hÆ°á»›ng vÃ  má»©c Ä‘á»™ thay Ä‘á»•i cáº§n thiáº¿t Ä‘á»ƒ giáº£m sai sá»‘.
    """)
    st.image("https://i.imgur.com/8Q4f5vR.png", 
             caption="Lan truyá»n ngÆ°á»£c: TÃ­nh gradient Ä‘á»ƒ Ä‘iá»u chá»‰nh.", width=400)

    st.markdown("""
    #### BÆ°á»›c 7: Cáº­p nháº­t trá»ng sá»‘
    - Sá»­ dá»¥ng thuáº­t toÃ¡n tá»‘i Æ°u (vÃ­ dá»¥: Gradient Descent) Ä‘á»ƒ Ä‘iá»u chá»‰nh trá»ng sá»‘ vÃ  bias:
      $$ W = W - \\eta \\cdot \\frac{\\partial L}{\\partial W} $$
      $$ b = b - \\eta \\cdot \\frac{\\partial L}{\\partial b} $$
    - \(\\eta\): Tá»‘c Ä‘á»™ há»c (learning rate), quyáº¿t Ä‘á»‹nh bÆ°á»›c cáº­p nháº­t lá»›n hay nhá».
    """)
    st.image("https://i.imgur.com/5n5sX7v.png", 
             caption="Cáº­p nháº­t trá»ng sá»‘ báº±ng Gradient Descent.", width=350)

    st.markdown("""
    #### BÆ°á»›c 8: Láº·p láº¡i quÃ¡ trÃ¬nh huáº¥n luyá»‡n
    - Láº·p qua toÃ n bá»™ dá»¯ liá»‡u nhiá»u láº§n (epochs), chia thÃ nh cÃ¡c batch nhá» Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ dáº§n dáº§n.
    - Sau má»—i láº§n láº·p, mÃ´ hÃ¬nh cáº£i thiá»‡n kháº£ nÄƒng dá»± Ä‘oÃ¡n báº±ng cÃ¡ch giáº£m hÃ m máº¥t mÃ¡t.
    """)
    st.image("https://i.imgur.com/6g6K9vN.png", 
             caption="Láº·p láº¡i quÃ¡ trÃ¬nh huáº¥n luyá»‡n qua nhiá»u epochs.", width=400)

    # Pháº§n 3: Vai trÃ² cá»§a cÃ¡c thÃ nh pháº§n
    st.markdown("""
    ### 3. Vai trÃ² cá»§a cÃ¡c thÃ nh pháº§n
    - **HÃ m kÃ­ch hoáº¡t**: Táº¡o tÃ­nh phi tuyáº¿n, giÃºp mÃ´ hÃ¬nh há»c cÃ¡c Ä‘áº·c trÆ°ng phá»©c táº¡p.
    - **Tá»‘c Ä‘á»™ há»c (Learning Rate)**: Quyáº¿t Ä‘á»‹nh bÆ°á»›c cáº­p nháº­t trá»ng sá»‘, áº£nh hÆ°á»Ÿng Ä‘áº¿n tá»‘c Ä‘á»™ vÃ  Ä‘á»™ á»•n Ä‘á»‹nh.
    - **Sá»‘ lá»›p vÃ  nÆ¡-ron**: TÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh, nhÆ°ng cáº§n cÃ¢n báº±ng Ä‘á»ƒ trÃ¡nh overfitting hoáº·c underfitting.
    """)

    # Pháº§n 4: á»¨ng dá»¥ng
    st.markdown("""
    ### 4. á»¨ng dá»¥ng
    - **PhÃ¢n loáº¡i**: Nháº­n diá»‡n hÃ¬nh áº£nh, vÄƒn báº£n (vÃ­ dá»¥: chá»¯ sá»‘ viáº¿t tay).
    - **Há»“i quy**: Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c (vÃ­ dá»¥: giÃ¡ nhÃ ).
    - **Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, thá»‹ giÃ¡c mÃ¡y tÃ­nh**: DÃ¹ng máº¡ng sÃ¢u (Deep Neural Networks).
    """)

    # Pháº§n 5: KhÃ¡i niá»‡m quan trá»ng
    st.markdown("""
    ### 5. Má»™t sá»‘ khÃ¡i niá»‡m quan trá»ng
    - **Overfitting**: MÃ´ hÃ¬nh há»c quÃ¡ tá»‘t trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n, nhÆ°ng kÃ©m trÃªn dá»¯ liá»‡u má»›i.
    - **Underfitting**: MÃ´ hÃ¬nh khÃ´ng há»c Ä‘á»§, dá»± Ä‘oÃ¡n kÃ©m trÃªn cáº£ dá»¯ liá»‡u huáº¥n luyá»‡n.
    - **Regularization**: Ká»¹ thuáº­t (nhÆ° Dropout) Ä‘á»ƒ giáº£m overfitting.
    """)

    # ThÃªm script MathJax Ä‘á»ƒ hiá»ƒn thá»‹ cÃ´ng thá»©c toÃ¡n há»c
    st.markdown("""
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    """, unsafe_allow_html=True)

# Tab Huáº¥n luyá»‡n
with tab2:  # Giá»¯ nguyÃªn 'with tab2:' náº¿u báº¡n Ä‘ang dÃ¹ng tab
    st.header("1. Chá»n kÃ­ch thÆ°á»›c vÃ  chia táº­p dá»¯ liá»‡u")
    
    # Khá»Ÿi táº¡o tráº¡ng thÃ¡i dá»¯ liá»‡u
    if "mnist_loaded" not in st.session_state:
        mnist = fetch_openml('mnist_784', version=1, as_frame=False)
        st.session_state.total_samples = mnist.data.shape[0]
        st.session_state.mnist_data = mnist
        st.session_state.mnist_loaded = False
        st.session_state.data_split_done = False

    sample_size = st.number_input(
        "Chá»n sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u", 
        min_value=1000, 
        max_value=st.session_state.total_samples, 
        value=10000, 
        step=1000,
        help="Sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u Ä‘Æ°á»£c láº¥y tá»« MNIST (tá»‘i Ä‘a 70,000). GiÃ¡ trá»‹ lá»›n hÆ¡n sáº½ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c nhÆ°ng cáº§n nhiá»u thá»i gian huáº¥n luyá»‡n hÆ¡n."
    )
    
    # Chá»n tá»· lá»‡ táº­p Test vÃ  Validation
    test_size = st.slider(
        "Chá»n tá»· lá»‡ dá»¯ liá»‡u Test", 
        0.1, 0.5, 0.2, 0.05,
        help="Tá»· lá»‡ dá»¯ liá»‡u dÃ¹ng Ä‘á»ƒ kiá»ƒm tra mÃ´ hÃ¬nh (10%-50%). NÃªn chá»n khoáº£ng 20%-30% Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ mÃ  khÃ´ng lÃ m giáº£m dá»¯ liá»‡u huáº¥n luyá»‡n."
    )
    valid_size = st.slider(
        "Chá»n tá»· lá»‡ dá»¯ liá»‡u Validation tá»« Train", 
        0.1, 0.3, 0.2, 0.05,
        help="Tá»· lá»‡ dá»¯ liá»‡u tá»« táº­p Train dÃ¹ng Ä‘á»ƒ kiá»ƒm tra trong lÃºc huáº¥n luyá»‡n (10%-30%). GiÃºp Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh mÃ  khÃ´ng dÃ¹ng táº­p Test."
    )

    if st.button("Chia tÃ¡ch dá»¯ liá»‡u"):
        mnist = st.session_state.mnist_data
        X, y = mnist.data / 255.0, mnist.target.astype(int)
        
        if sample_size < st.session_state.total_samples:
            X, _, y, _ = train_test_split(X, y, train_size=sample_size, random_state=42, stratify=y)
        
        # LÆ°u dá»¯ liá»‡u gá»‘c vÃ o session_state
        st.session_state.X = X
        st.session_state.y = y

        # Chia dá»¯ liá»‡u thÃ nh Train_full vÃ  Test
        X_train_full, X_test, y_train_full, y_test = train_test_split(
            st.session_state.X, st.session_state.y, test_size=test_size, random_state=42, stratify=st.session_state.y
        )
        # Chia Train_full thÃ nh Train vÃ  Validation
        X_train, X_valid, y_train, y_valid = train_test_split(
            X_train_full, y_train_full, test_size=valid_size, random_state=42, stratify=y_train_full
        )

        # LÆ°u vÃ o session_state
        st.session_state.X_train = X_train
        st.session_state.X_valid = X_valid
        st.session_state.X_test = X_test
        st.session_state.y_train = y_train
        st.session_state.y_valid = y_valid
        st.session_state.y_test = y_test
        st.session_state.data_split_done = True
        st.session_state.mnist_loaded = True

        # Hiá»ƒn thá»‹ thÃ´ng tin sau khi chia tÃ¡ch
        st.write(f"Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chia tÃ¡ch vá»›i {sample_size} máº«u!")
        st.write(f"- Dá»¯ liá»‡u Train: {st.session_state.X_train.shape} ({(1-test_size)*(1-valid_size)*100:.1f}%)")
        st.write(f"- Dá»¯ liá»‡u Validation: {st.session_state.X_valid.shape} ({(1-test_size)*valid_size*100:.1f}%)")
        st.write(f"- Dá»¯ liá»‡u Test: {st.session_state.X_test.shape} ({test_size*100:.1f}%)")

    # Hiá»ƒn thá»‹ vÃ­ dá»¥ hÃ¬nh áº£nh
    st.subheader("VÃ­ dá»¥ hÃ¬nh áº£nh tá»« táº­p Train")
    if st.session_state.get("data_split_done", False):
        X = st.session_state.X_train
        y = st.session_state.y_train
        indices = random.sample(range(len(X)), 5)
        fig, axs = plt.subplots(1, 5, figsize=(12, 3))
        for i, idx in enumerate(indices):
            img = X[idx].reshape(28, 28)
            axs[i].imshow(img, cmap='gray')
            axs[i].axis('off')
            axs[i].set_title(f"Label: {y[idx]}")
        st.pyplot(fig)

    # Tab Huáº¥n luyá»‡n
    st.header("Huáº¥n luyá»‡n Neural Network")

    # CÃ¡c tham sá»‘ cÆ¡ báº£n thÃ´ng dá»¥ng cho ngÆ°á»i dÃ¹ng lá»±a chá»n
    st.subheader("Cáº¥u hÃ¬nh huáº¥n luyá»‡n")
    num_epochs = st.number_input(
        "Sá»‘ epochs", 
        min_value=1, 
        max_value=50, 
        value=10,
        help="Sá»‘ láº§n mÃ´ hÃ¬nh há»c qua toÃ n bá»™ dá»¯ liá»‡u. TÄƒng giÃ¡ trá»‹ Ä‘á»ƒ há»c tá»‘t hÆ¡n, nhÆ°ng quÃ¡ nhiá»u cÃ³ thá»ƒ gÃ¢y overfitting."
    )
    batch_size = st.selectbox(
        "Batch size", 
        [16, 32, 64, 128, 256, 512], 
        index=1,  # Máº·c Ä‘á»‹nh 32
        help="Sá»‘ máº«u xá»­ lÃ½ cÃ¹ng lÃºc. GiÃ¡ trá»‹ nhá» tÄƒng Ä‘á»™ chÃ­nh xÃ¡c nhÆ°ng cháº­m hÆ¡n; giÃ¡ trá»‹ lá»›n tÄƒng tá»‘c Ä‘á»™ nhÆ°ng cáº§n bá»™ nhá»› lá»›n."
    )
    learning_rate = st.number_input(
        "Tá»‘c Ä‘á»™ há»c (learning rate)", 
        min_value=0.0001, 
        max_value=0.1, 
        value=0.001, 
        step=0.0001,
        help="Kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ há»c cá»§a mÃ´ hÃ¬nh. GiÃ¡ trá»‹ nhá» há»c cháº­m nhÆ°ng á»•n Ä‘á»‹nh; giÃ¡ trá»‹ lá»›n há»c nhanh nhÆ°ng cÃ³ thá»ƒ khÃ´ng há»™i tá»¥."
    )
    hidden_neurons = st.selectbox(
        "Sá»‘ nÆ¡-ron lá»›p áº©n", 
        [32, 64, 128, 256, 512], 
        index=2,  # Máº·c Ä‘á»‹nh 128
        help="Sá»‘ nÆ¡-ron trong lá»›p áº©n. GiÃ¡ trá»‹ lá»›n tÄƒng kháº£ nÄƒng há»c Ä‘áº·c trÆ°ng phá»©c táº¡p, nhÆ°ng quÃ¡ nhiá»u cÃ³ thá»ƒ gÃ¢y overfitting."
    )
    activation_function = st.selectbox(
        "HÃ m kÃ­ch hoáº¡t (Activation Function)",
        ["ReLU", "Sigmoid", "Tanh"],
        index=0,  # Máº·c Ä‘á»‹nh ReLU
        help="HÃ m biáº¿n Ä‘á»•i Ä‘áº§u ra cá»§a lá»›p áº©n. ReLU phá»• biáº¿n vÃ  nhanh; Sigmoid phÃ¹ há»£p vá»›i giÃ¡ trá»‹ 0-1; Tanh cÃ¢n báº±ng quanh 0."
    )

    # Nháº­p tÃªn cho thÃ­ nghiá»‡m MLflow
    experiment_name = st.text_input(
        "Nháº­p tÃªn cho thÃ­ nghiá»‡m MLflow", 
        value="",
        help="TÃªn Ä‘á»ƒ lÆ°u thÃ­ nghiá»‡m trong MLflow. Náº¿u Ä‘á»ƒ trá»‘ng, há»‡ thá»‘ng sáº½ tá»± táº¡o tÃªn dá»±a trÃªn thá»i gian."
    )
    if not experiment_name:
        timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        experiment_name = f"Neural_Network_MNIST_{timestamp}"
    
    # NÃºt Ä‘á»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n
    if st.button("Huáº¥n luyá»‡n mÃ´ hÃ¬nh"):
        # Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chia tÃ¡ch chÆ°a
        if not st.session_state.get("data_split_done", False):
            st.error("Vui lÃ²ng chia tÃ¡ch dá»¯ liá»‡u trÆ°á»›c!")
        else:
            # Láº¥y dá»¯ liá»‡u tá»« session state
            X_train = st.session_state.X_train
            y_train = st.session_state.y_train
            X_valid = st.session_state.X_valid
            y_valid = st.session_state.y_valid
            X_test = st.session_state.X_test
            y_test = st.session_state.y_test

            # Chuyá»ƒn dá»¯ liá»‡u sang tensor
            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
            y_train_tensor = torch.tensor(y_train, dtype=torch.long)
            X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)
            y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)
            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
            y_test_tensor = torch.tensor(y_test, dtype=torch.long)

            # Táº¡o DataLoader
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)
            valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)
            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

            # Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Neural Network vá»›i activation tÃ¹y chá»‰nh
            class SimpleNN(nn.Module):
                def __init__(self, hidden_size=hidden_neurons, activation=activation_function):
                    super(SimpleNN, self).__init__()
                    self.fc1 = nn.Linear(784, hidden_size)  # Input: 784, Hidden: tÃ¹y chá»‰nh
                    # Chá»n hÃ m kÃ­ch hoáº¡t dá»±a trÃªn lá»±a chá»n cá»§a ngÆ°á»i dÃ¹ng
                    if activation == "ReLU":
                        self.activation = nn.ReLU()
                    elif activation == "Sigmoid":
                        self.activation = nn.Sigmoid()
                    elif activation == "Tanh":
                        self.activation = nn.Tanh()
                    self.fc2 = nn.Linear(hidden_size, 10)   # Hidden: tÃ¹y chá»‰nh, Output: 10

                def forward(self, x):
                    x = self.fc1(x)
                    x = self.activation(x)
                    x = self.fc2(x)
                    return x

            # Khá»Ÿi táº¡o mÃ´ hÃ¬nh, loss vÃ  optimizer
            model = SimpleNN(hidden_size=hidden_neurons, activation=activation_function)
            criterion = nn.CrossEntropyLoss()           # Cá»‘ Ä‘á»‹nh CrossEntropyLoss
            optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Cá»‘ Ä‘á»‹nh Adam

            # Thiáº¿t láº­p MLflow
            mlflow.set_experiment(experiment_name)
            with mlflow.start_run():
                # Log cÃ¡c tham sá»‘
                mlflow.log_param("num_epochs", num_epochs)
                mlflow.log_param("batch_size", batch_size)
                mlflow.log_param("learning_rate", learning_rate)
                mlflow.log_param("hidden_neurons", hidden_neurons)
                mlflow.log_param("activation_function", activation_function)
                mlflow.log_param("test_size", test_size)
                mlflow.log_param("valid_size", valid_size)

                # Thanh tiáº¿n trÃ¬nh vÃ  tráº¡ng thÃ¡i
                progress_bar = st.progress(0)
                status_text = st.empty()

                # Danh sÃ¡ch Ä‘á»ƒ lÆ°u Ä‘á»™ chÃ­nh xÃ¡c
                train_acc_history = []
                valid_acc_history = []

                # Huáº¥n luyá»‡n mÃ´ hÃ¬nh
                for epoch in range(num_epochs):
                    model.train()
                    correct = 0
                    total = 0
                    for inputs, labels in train_loader:
                        optimizer.zero_grad()
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        _, predicted = torch.max(outputs.data, 1)
                        total += labels.size(0)
                        correct += (predicted == labels).sum().item()
                    train_acc = correct / total
                    train_acc_history.append(train_acc)

                    # ÄÃ¡nh giÃ¡ trÃªn táº­p validation
                    model.eval()
                    correct = 0
                    total = 0
                    with torch.no_grad():
                        for inputs, labels in valid_loader:
                            outputs = model(inputs)
                            _, predicted = torch.max(outputs.data, 1)
                            total += labels.size(0)
                            correct += (predicted == labels).sum().item()
                    valid_acc = correct / total
                    valid_acc_history.append(valid_acc)

                    # Log metrics vÃ o MLflow
                    mlflow.log_metric("train_accuracy", train_acc, step=epoch)
                    mlflow.log_metric("valid_accuracy", valid_acc, step=epoch)

                    # Cáº­p nháº­t thanh tiáº¿n trÃ¬nh vÃ  thÃ´ng tin
                    progress = (epoch + 1) / num_epochs
                    progress_bar.progress(progress)
                    status_text.text(f"Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_acc:.4f}, Validation Accuracy: {valid_acc:.4f}")

                # LÆ°u mÃ´ hÃ¬nh vÃ o MLflow
                mlflow.pytorch.log_model(model, "model")

                # Hiá»ƒn thá»‹ thÃ´ng bÃ¡o hoÃ n táº¥t
                st.success("Huáº¥n luyá»‡n hoÃ n táº¥t!")

                # Váº½ sÆ¡ Ä‘á»“ cáº¥u trÃºc cÃ¡c lá»›p cá»§a mÃ´ hÃ¬nh vá»›i kÃ­ch thÆ°á»›c tá»· lá»‡
                st.subheader("SÆ¡ Ä‘á»“ cáº¥u trÃºc cÃ¡c lá»›p cá»§a mÃ´ hÃ¬nh")
                fig, ax = plt.subplots(figsize=(12, 5))

                # Äá»‹nh nghÄ©a kÃ­ch thÆ°á»›c cÃ¡c lá»›p
                model_dims = {
                    "Input Layer": 784,
                    f"Hidden Layer\n({activation_function})": hidden_neurons,
                    "Output Layer": 10
                }

                # Vá»‹ trÃ­ cá»§a cÃ¡c lá»›p trÃªn trá»¥c x
                x_positions = [0, 3, 5]

                # TÃ­nh chiá»u cao tá»· lá»‡ dá»±a trÃªn sá»‘ nÆ¡-ron (log scale Ä‘á»ƒ trÃ¡nh quÃ¡ chÃªnh lá»‡ch)
                max_height = 2.0  # Chiá»u cao tá»‘i Ä‘a cá»§a hÃ¬nh chá»¯ nháº­t
                heights = [min(max_height, max_height * size / 784) for size in model_dims.values()]

                # Váº½ cÃ¡c lá»›p dÆ°á»›i dáº¡ng hÃ¬nh chá»¯ nháº­t vá»›i chiá»u cao tá»· lá»‡
                for i, (layer_name, size) in enumerate(model_dims.items()):
                    rect = patches.Rectangle(
                        (x_positions[i] - 0.4, -heights[i]/2),  # Vá»‹ trÃ­ (x, y)
                        0.8, heights[i],  # Chiá»u rá»™ng vÃ  chiá»u cao
                        linewidth=1, edgecolor='black', facecolor='lightblue'
                    )
                    ax.add_patch(rect)
                    ax.text(x_positions[i], heights[i]/2 + 0.2, f"{layer_name}\n{size} nÆ¡-ron", 
                            ha='center', va='bottom', fontsize=12)

                # Váº½ mÅ©i tÃªn káº¿t ná»‘i cÃ¡c lá»›p
                for i in range(len(x_positions) - 1):
                    ax.arrow(x_positions[i] + 0.4, 0, x_positions[i+1] - x_positions[i] - 0.8, 0, 
                             head_width=0.1, head_length=0.1, fc='black', ec='black')

                # TÃ¹y chá»‰nh biá»ƒu Ä‘á»“
                ax.set_xlim(-1, 6)
                ax.set_ylim(-max_height/2 - 0.5, max_height/2 + 0.8)
                ax.axis('off')  # Táº¯t trá»¥c Ä‘á»ƒ sÆ¡ Ä‘á»“ trÃ´ng gá»n gÃ ng
                st.pyplot(fig)

                # Váº½ biá»ƒu Ä‘á»“ huáº¥n luyá»‡n
                st.subheader("Biá»ƒu Ä‘á»“ Ä‘á»™ chÃ­nh xÃ¡c qua cÃ¡c epoch")
                fig, ax = plt.subplots()
                ax.plot(range(1, num_epochs+1), train_acc_history, label='Train Accuracy')
                ax.plot(range(1, num_epochs+1), valid_acc_history, label='Validation Accuracy')
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Accuracy')
                ax.legend()
                st.pyplot(fig)
# Tab 3: MLflow
with tab3:
    st.header("Tracking MLflow")
    try:
        from mlflow.tracking import MlflowClient
        client = MlflowClient()

        # Láº¥y danh sÃ¡ch thÃ­ nghiá»‡m tá»« MLflow
        experiments = mlflow.search_experiments()

        if experiments:
            st.write("#### Danh sÃ¡ch thÃ­ nghiá»‡m")
            experiment_data = [
                {
                    "Experiment ID": exp.experiment_id,
                    "Experiment Name": exp.name,
                    "Artifact Location": exp.artifact_location
                }
                for exp in experiments
            ]
            df_experiments = pd.DataFrame(experiment_data)
            st.dataframe(df_experiments)

            # Chá»n thÃ­ nghiá»‡m dá»±a trÃªn TÃŠN thay vÃ¬ ID
            selected_exp_name = st.selectbox(
                "ğŸ” Chá»n thÃ­ nghiá»‡m Ä‘á»ƒ xem chi tiáº¿t",
                options=[exp.name for exp in experiments]
            )

            # Láº¥y ID tÆ°Æ¡ng á»©ng vá»›i tÃªn Ä‘Æ°á»£c chá»n
            selected_exp_id = next(exp.experiment_id for exp in experiments if exp.name == selected_exp_name)

            # Láº¥y danh sÃ¡ch runs trong thÃ­ nghiá»‡m Ä‘Ã£ chá»n
            runs = mlflow.search_runs(selected_exp_id)
            if not runs.empty:
                st.write("#### Danh sÃ¡ch runs")
                st.dataframe(runs)

                # Chá»n run Ä‘á»ƒ xem chi tiáº¿t
                selected_run_id = st.selectbox(
                    "ğŸ” Chá»n run Ä‘á»ƒ xem chi tiáº¿t",
                    options=runs["run_id"]
                )

                # Hiá»ƒn thá»‹ chi tiáº¿t run
                run = mlflow.get_run(selected_run_id)
                st.write("##### ThÃ´ng tin run")
                st.write(f"*Run ID:* {run.info.run_id}")
                st.write(f"*Experiment ID:* {run.info.experiment_id}")
                st.write(f"*Start Time:* {run.info.start_time}")

                # Hiá»ƒn thá»‹ metrics
                st.write("##### Metrics")
                st.json(run.data.metrics)

                # Hiá»ƒn thá»‹ params
                st.write("##### Params")
                st.json(run.data.params)

                # Hiá»ƒn thá»‹ artifacts sá»­ dá»¥ng client.list_artifacts
                artifacts = client.list_artifacts(selected_run_id)
                if artifacts:
                    st.write("##### Artifacts")
                    for artifact in artifacts:
                        st.write(f"- {artifact.path}")
            else:
                st.warning("KhÃ´ng cÃ³ runs nÃ o trong thÃ­ nghiá»‡m nÃ y.")
        else:
            st.warning("KhÃ´ng cÃ³ thÃ­ nghiá»‡m nÃ o Ä‘Æ°á»£c tÃ¬m tháº¥y.")
    except Exception as e:
        st.error(f"ÄÃ£ xáº£y ra lá»—i khi láº¥y danh sÃ¡ch thÃ­ nghiá»‡m: {e}")