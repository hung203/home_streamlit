import datetime
import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
import mlflow
import mlflow.pytorch
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import datetime
import random
from turtle import getcanvas
import cv2
from matplotlib import patches
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st
import mlflow
import mlflow.pytorch
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from PIL import Image
from streamlit_drawable_canvas import st_canvas
# Ti√™u ƒë·ªÅ ·ª©ng d·ª•ng
st.title("Ph√¢n lo·∫°i ch·ªØ s·ªë vi·∫øt tay MNIST v·ªõi Self-Training Neural Network")

# T·∫°o c√°c tab
tab1, tab2, tab3, tab4 = st.tabs(["L√Ω thuy·∫øt", "Hu·∫•n luy·ªán", "D·ª± ƒêo√°n", "MLflow"])

# Tab 2: Hu·∫•n luy·ªán
with tab2:
    st.header("1. Ch·ªçn k√≠ch th∆∞·ªõc v√† chia t·∫≠p d·ªØ li·ªáu")

    # Kh·ªüi t·∫°o tr·∫°ng th√°i d·ªØ li·ªáu
    if "mnist_loaded" not in st.session_state:
        mnist = fetch_openml('mnist_784', version=1, as_frame=False)
        st.session_state.total_samples = mnist.data.shape[0]
        st.session_state.mnist_data = mnist
        st.session_state.mnist_loaded = False
        st.session_state.data_split_done = False

    sample_size = st.number_input(
        "Ch·ªçn s·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu",
        min_value=1000,
        max_value=st.session_state.total_samples,
        value=10000,
        step=1000,
        help="S·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu ƒë∆∞·ª£c l·∫•y t·ª´ MNIST (t·ªëi ƒëa 70,000)."
    )

    test_size = st.slider(
        "Ch·ªçn t·ª∑ l·ªá d·ªØ li·ªáu Test",
        0.1, 0.5, 0.2, 0.05,
        help="T·ª∑ l·ªá d·ªØ li·ªáu d√πng ƒë·ªÉ ki·ªÉm tra m√¥ h√¨nh (10%-50%)."
    )
    valid_size = st.slider(
        "Ch·ªçn t·ª∑ l·ªá d·ªØ li·ªáu Validation t·ª´ Train",
        0.1, 0.3, 0.2, 0.05,
        help="T·ª∑ l·ªá d·ªØ li·ªáu t·ª´ t·∫≠p Train d√πng ƒë·ªÉ ki·ªÉm tra trong l√∫c hu·∫•n luy·ªán."
    )

    if st.button("Chia t√°ch d·ªØ li·ªáu"):
        mnist = st.session_state.mnist_data
        X, y = mnist.data / 255.0, mnist.target.astype(int)

        if sample_size < st.session_state.total_samples:
            X, _, y, _ = train_test_split(X, y, train_size=sample_size / st.session_state.total_samples, random_state=42, stratify=y)

        st.session_state.X = X
        st.session_state.y = y

        # Chia t·∫≠p Train v√† Test
        X_train_full, X_test, y_train_full, y_test = train_test_split(
            st.session_state.X, st.session_state.y, test_size=test_size, random_state=42, stratify=st.session_state.y
        )

        # Chia t·∫≠p Train th√†nh Train v√† Validation
        X_train, X_valid, y_train, y_valid = train_test_split(
            X_train_full, y_train_full, test_size=valid_size, random_state=42, stratify=y_train_full
        )

        # T·ª´ t·∫≠p Train, l·∫•y 1% m·ªói l·ªõp l√†m t·∫≠p labeled ban ƒë·∫ßu
        X_labeled = []
        y_labeled = []
        X_unlabeled = []
        y_unlabeled = []
        for digit in range(10):
            digit_indices = np.where(y_train == digit)[0]
            num_samples = len(digit_indices)
            num_labeled = max(1, int(num_samples * 0.01))  # L·∫•y 1%, ƒë·∫£m b·∫£o √≠t nh·∫•t 1 m·∫´u
            labeled_indices = np.random.choice(digit_indices, num_labeled, replace=False)
            unlabeled_indices = np.setdiff1d(digit_indices, labeled_indices)

            X_labeled.append(X_train[labeled_indices])
            y_labeled.append(y_train[labeled_indices])
            X_unlabeled.append(X_train[unlabeled_indices])
            y_unlabeled.append(y_train[unlabeled_indices])

        X_labeled = np.concatenate(X_labeled)
        y_labeled = np.concatenate(y_labeled)
        X_unlabeled = np.concatenate(X_unlabeled)
        y_unlabeled = np.concatenate(y_unlabeled)  # Ground truth cho ƒë√°nh gi√°

        # L∆∞u v√†o session_state
        st.session_state.X_train_labeled = X_labeled
        st.session_state.y_train_labeled = y_labeled
        st.session_state.X_train_unlabeled = X_unlabeled
        st.session_state.y_train_unlabeled = y_unlabeled
        st.session_state.X_valid = X_valid
        st.session_state.y_valid = y_valid
        st.session_state.X_test = X_test
        st.session_state.y_test = y_test
        st.session_state.data_split_done = True
        st.session_state.mnist_loaded = True

        st.write(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia t√°ch v·ªõi {sample_size} m·∫´u!")
        st.write(f"- D·ªØ li·ªáu Train t·ªïng: {X_train.shape} ({(1 - test_size) * (1 - valid_size) * 100:.1f}%)")
        st.write(f"  + Train c√≥ nh√£n (1% m·ªói l·ªõp): {X_labeled.shape}")
        st.write(f"  + Train kh√¥ng nh√£n: {X_unlabeled.shape}")
        st.write(f"- D·ªØ li·ªáu Validation: {X_valid.shape} ({(1 - test_size) * valid_size * 100:.1f}%)")
        st.write(f"- D·ªØ li·ªáu Test: {X_test.shape} ({test_size * 100:.1f}%)")

    # C·∫•u h√¨nh hu·∫•n luy·ªán Self-Training
    st.header("2. Hu·∫•n luy·ªán Neural Network v·ªõi Self-Training")
    st.subheader("C·∫•u h√¨nh hu·∫•n luy·ªán")
    num_epochs = st.number_input("S·ªë epochs m·ªói v√≤ng", min_value=1, max_value=50, value=10)
    batch_size = st.selectbox("Batch size", [16, 32, 64, 128], index=1)
    learning_rate = st.number_input("T·ªëc ƒë·ªô h·ªçc", min_value=0.0001, max_value=0.1, value=0.001, step=0.0001)
    num_hidden_layers = st.selectbox("S·ªë l·ªõp ·∫©n", [1, 2, 3], index=0)
    hidden_neurons = st.selectbox("S·ªë n∆°-ron m·ªói l·ªõp ·∫©n", [64, 128, 256], index=1)
    activation_function = st.selectbox("H√†m k√≠ch ho·∫°t", ["ReLU", "Sigmoid", "Tanh"], index=0)
    threshold = st.slider("Ng∆∞·ª°ng g√°n Pseudo Label", 0.5, 0.99, 0.95, 0.01)
    max_iterations = st.number_input("S·ªë v√≤ng l·∫∑p t·ªëi ƒëa", min_value=1, max_value=20, value=5)

    experiment_name = st.text_input(
        "Nh·∫≠p t√™n cho th√≠ nghi·ªám MLflow",
        value=f"Self_Training_MNIST_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}"
    )

    if st.button("B·∫Øt ƒë·∫ßu Self-Training"):
        if not st.session_state.get("data_split_done", False):
            st.error("Vui l√≤ng chia t√°ch d·ªØ li·ªáu tr∆∞·ªõc!")
        else:
            X_labeled = st.session_state.X_train_labeled
            y_labeled = st.session_state.y_train_labeled
            X_unlabeled = st.session_state.X_train_unlabeled
            y_unlabeled = st.session_state.y_train_unlabeled  # Ground truth
            X_valid = st.session_state.X_valid
            y_valid = st.session_state.y_valid
            X_test = st.session_state.X_test
            y_test = st.session_state.y_test

            # ƒê·ªãnh nghƒ©a m√¥ h√¨nh Neural Network
            class SimpleNN(nn.Module):
                def __init__(self, num_hidden_layers, hidden_size, activation):
                    super(SimpleNN, self).__init__()
                    layers = [nn.Linear(784, hidden_size)]
                    if activation == "ReLU":
                        layers.append(nn.ReLU())
                    elif activation == "Sigmoid":
                        layers.append(nn.Sigmoid())
                    elif activation == "Tanh":
                        layers.append(nn.Tanh())
                    for _ in range(num_hidden_layers - 1):
                        layers.append(nn.Linear(hidden_size, hidden_size))
                        if activation == "ReLU":
                            layers.append(nn.ReLU())
                        elif activation == "Sigmoid":
                            layers.append(nn.Sigmoid())
                        elif activation == "Tanh":
                            layers.append(nn.Tanh())
                    layers.append(nn.Linear(hidden_size, 10))
                    self.network = nn.Sequential(*layers)

                def forward(self, x):
                    return self.network(x)

            # Thi·∫øt l·∫≠p MLflow
            mlflow.set_experiment(experiment_name)
            with mlflow.start_run() as run:
                # Log c√°c tham s·ªë
                mlflow.log_param("num_epochs", num_epochs)
                mlflow.log_param("batch_size", batch_size)
                mlflow.log_param("learning_rate", learning_rate)
                mlflow.log_param("num_hidden_layers", num_hidden_layers)
                mlflow.log_param("hidden_neurons", hidden_neurons)
                mlflow.log_param("activation_function", activation_function)
                mlflow.log_param("threshold", threshold)
                mlflow.log_param("max_iterations", max_iterations)
                mlflow.log_param("test_size", test_size)
                mlflow.log_param("valid_size", valid_size)
                mlflow.log_param("sample_size", sample_size)

                progress_bar = st.progress(0)
                status_text = st.empty()
                test_acc_history = []
                valid_acc_history = []

                # V√≤ng l·∫∑p Self-Training
                for iteration in range(max_iterations):
                    # (2) Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n t·∫≠p labeled
                    model = SimpleNN(num_hidden_layers, hidden_neurons, activation_function)
                    criterion = nn.CrossEntropyLoss()
                    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

                    X_labeled_tensor = torch.tensor(X_labeled, dtype=torch.float32)
                    y_labeled_tensor = torch.tensor(y_labeled, dtype=torch.long)
                    train_dataset = TensorDataset(X_labeled_tensor, y_labeled_tensor)
                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

                    model.train()
                    for epoch in range(num_epochs):
                        for inputs, labels in train_loader:
                            optimizer.zero_grad()
                            outputs = model(inputs)
                            loss = criterion(outputs, labels)
                            loss.backward()
                            optimizer.step()

                    # (3) D·ª± ƒëo√°n nh√£n cho t·∫≠p unlabeled
                    model.eval()
                    X_unlabeled_tensor = torch.tensor(X_unlabeled, dtype=torch.float32)
                    with torch.no_grad():
                        outputs = model(X_unlabeled_tensor)
                        probabilities = torch.softmax(outputs, dim=1).numpy()
                        predictions = np.argmax(probabilities, axis=1)
                        max_probs = np.max(probabilities, axis=1)

                    # (4) G√°n Pseudo Label v·ªõi ng∆∞·ª°ng
                    pseudo_mask = max_probs >= threshold
                    X_pseudo = X_unlabeled[pseudo_mask]
                    y_pseudo = predictions[pseudo_mask]

                    # (5) C·∫≠p nh·∫≠t t·∫≠p labeled
                    X_labeled = np.concatenate([X_labeled, X_pseudo])
                    y_labeled = np.concatenate([y_labeled, y_pseudo])
                    X_unlabeled = X_unlabeled[~pseudo_mask]

                    # ƒê√°nh gi√° tr√™n t·∫≠p Validation
                    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)
                    y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)
                    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)
                    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)
                    correct = 0
                    total = 0
                    with torch.no_grad():
                        for inputs, labels in valid_loader:
                            outputs = model(inputs)
                            _, predicted = torch.max(outputs.data, 1)
                            total += labels.size(0)
                            correct += (predicted == labels).sum().item()
                    valid_acc = correct / total
                    valid_acc_history.append(valid_acc)

                    # ƒê√°nh gi√° tr√™n t·∫≠p Test
                    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
                    y_test_tensor = torch.tensor(y_test, dtype=torch.long)
                    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
                    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                    correct = 0
                    total = 0
                    with torch.no_grad():
                        for inputs, labels in test_loader:
                            outputs = model(inputs)
                            _, predicted = torch.max(outputs.data, 1)
                            total += labels.size(0)
                            correct += (predicted == labels).sum().item()
                    test_acc = correct / total
                    test_acc_history.append(test_acc)

                    # Log k·∫øt qu·∫£
                    mlflow.log_metric("labeled_size", len(X_labeled), step=iteration)
                    mlflow.log_metric("unlabeled_size", len(X_unlabeled), step=iteration)
                    mlflow.log_metric("valid_accuracy", valid_acc, step=iteration)
                    mlflow.log_metric("test_accuracy", test_acc, step=iteration)

                    progress = (iteration + 1) / max_iterations
                    progress_bar.progress(progress)
                    status_text.text(f"Iteration {iteration+1}/{max_iterations}, Labeled: {len(X_labeled)}, Valid Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}")

                    # D·ª´ng n·∫øu kh√¥ng c√≤n d·ªØ li·ªáu unlabeled
                    if len(X_unlabeled) == 0:
                        st.write("ƒê√£ g√°n nh√£n h·∫øt d·ªØ li·ªáu kh√¥ng nh√£n!")
                        break

                # L∆∞u m√¥ h√¨nh
                mlflow.pytorch.log_model(model, "model")
                st.session_state.model = model
                st.session_state.run_id = run.info.run_id

                st.success("Qu√° tr√¨nh Self-Training ho√†n t·∫•t!")

                # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ti·∫øn tr√¨nh
                st.subheader("Ti·∫øn tr√¨nh Self-Training")
                fig, ax = plt.subplots()
                ax.plot(range(1, len(test_acc_history) + 1), test_acc_history, label="Test Accuracy")
                ax.plot(range(1, len(valid_acc_history) + 1), valid_acc_history, label="Validation Accuracy")
                ax.set_xlabel("Iteration")
                ax.set_ylabel("Accuracy")
                ax.legend()
                st.pyplot(fig)

# C√°c tab kh√°c (gi·ªØ nguy√™n ho·∫∑c ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn)
with tab1:
    st.header("L√Ω thuy·∫øt")
    st.write("N·ªôi dung l√Ω thuy·∫øt ·ªü ƒë√¢y...")

with tab3:
    # H√†m ti·ªÅn x·ª≠ l√Ω ·∫£nh t·∫£i l√™n
    def preprocess_uploaded_image(image):
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        image = cv2.resize(image, (28, 28))
        image = image.reshape(1, -1) / 255.0
        return image

    # H√†m ti·ªÅn x·ª≠ l√Ω ·∫£nh t·ª´ canvas
    def preprocess_canvas_image(image_data):
        image = np.array(image_data)[:, :, 0]  # L·∫•y k√™nh grayscale
        image = cv2.resize(image, (28, 28))
        image = image.reshape(1, -1) / 255.0
        return image

    # Ki·ªÉm tra m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ch∆∞a
    if "model" not in st.session_state:
        st.error("‚ö†Ô∏è M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán! H√£y quay l·∫°i tab 'Chia d·ªØ li·ªáu & Hu·∫•n luy·ªán' ƒë·ªÉ hu·∫•n luy·ªán tr∆∞·ªõc.")
        st.stop()

    st.header("üñçÔ∏è D·ª± ƒëo√°n ch·ªØ s·ªë vi·∫øt tay")
    option = st.radio("üñºÔ∏è Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p:", ["üìÇ T·∫£i ·∫£nh l√™n", "‚úèÔ∏è V·∫Ω s·ªë"])

    if option == "üìÇ T·∫£i ·∫£nh l√™n":
        uploaded_file = st.file_uploader("üì§ T·∫£i ·∫£nh s·ªë vi·∫øt tay (PNG, JPG)", type=["png", "jpg", "jpeg"])
        if uploaded_file is not None:
            image = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), cv2.IMREAD_COLOR)
            processed_image = preprocess_uploaded_image(image)
            st.image(image, caption="üì∑ ·∫¢nh t·∫£i l√™n", use_column_width=True)

            if st.button("üîÆ D·ª± ƒëo√°n"):
                model = st.session_state.model
                model.eval()
                with torch.no_grad():
                    input_tensor = torch.tensor(processed_image, dtype=torch.float32)
                    outputs = model(input_tensor)
                    probabilities = torch.softmax(outputs, dim=1).numpy()[0]
                    prediction = np.argmax(probabilities)
                    st.write(f"üéØ **D·ª± ƒëo√°n: {prediction}**")
                    st.write(f"üî¢ **ƒê·ªô tin c·∫≠y: {probabilities[prediction] * 100:.2f}%**")

    elif option == "‚úèÔ∏è V·∫Ω s·ªë":
        # S·ª≠ d·ª•ng st_canvas v·ªõi c√°c tham s·ªë h·ª£p l·ªá
        canvas_result = st_canvas(
            fill_color="rgba(255, 255, 255, 0.0)",  # M√†u t√¥ (trong su·ªët ƒë·ªÉ kh√¥ng t√¥ n·ªÅn)
            stroke_width=15,                        # ƒê·ªô d√†y n√©t v·∫Ω
            stroke_color="black",                   # M√†u n√©t v·∫Ω
            background_color="white",               # M√†u n·ªÅn canvas
            width=280,                              # Chi·ªÅu r·ªông
            height=280,                             # Chi·ªÅu cao
            drawing_mode="freedraw",                # Ch·∫ø ƒë·ªô v·∫Ω t·ª± do
            key="canvas"                            # Kh√≥a duy nh·∫•t
        )
        if st.button("üîÆ D·ª± ƒëo√°n"):
            if canvas_result.image_data is not None:
                processed_canvas = preprocess_canvas_image(canvas_result.image_data)
                model = st.session_state.model
                model.eval()
                with torch.no_grad():
                    input_tensor = torch.tensor(processed_canvas, dtype=torch.float32)
                    outputs = model(input_tensor)
                    probabilities = torch.softmax(outputs, dim=1).numpy()[0]
                    prediction = np.argmax(probabilities)
                    st.write(f"üéØ **D·ª± ƒëo√°n: {prediction}**")
                    st.write(f"üî¢ **ƒê·ªô tin c·∫≠y: {probabilities[prediction] * 100:.2f}%**")

# Tab 3: MLflow
with tab4:
    st.header("Tracking MLflow")
    try:
        from mlflow.tracking import MlflowClient
        client = MlflowClient()

        # L·∫•y danh s√°ch th√≠ nghi·ªám t·ª´ MLflow
        experiments = mlflow.search_experiments()

        if experiments:
            st.write("#### Danh s√°ch th√≠ nghi·ªám")
            experiment_data = [
                {
                    "Experiment ID": exp.experiment_id,
                    "Experiment Name": exp.name,
                    "Artifact Location": exp.artifact_location
                }
                for exp in experiments
            ]
            df_experiments = pd.DataFrame(experiment_data)
            st.dataframe(df_experiments)

            # Ch·ªçn th√≠ nghi·ªám d·ª±a tr√™n T√äN thay v√¨ ID
            selected_exp_name = st.selectbox(
                "üîç Ch·ªçn th√≠ nghi·ªám ƒë·ªÉ xem chi ti·∫øt",
                options=[exp.name for exp in experiments]
            )

            # L·∫•y ID t∆∞∆°ng ·ª©ng v·ªõi t√™n ƒë∆∞·ª£c ch·ªçn
            selected_exp_id = next(exp.experiment_id for exp in experiments if exp.name == selected_exp_name)

            # L·∫•y danh s√°ch runs trong th√≠ nghi·ªám ƒë√£ ch·ªçn
            runs = mlflow.search_runs(selected_exp_id)
            if not runs.empty:
                st.write("#### Danh s√°ch runs")
                st.dataframe(runs)

                # Ch·ªçn run ƒë·ªÉ xem chi ti·∫øt
                selected_run_id = st.selectbox(
                    "üîç Ch·ªçn run ƒë·ªÉ xem chi ti·∫øt",
                    options=runs["run_id"]
                )

                # Hi·ªÉn th·ªã chi ti·∫øt run
                run = mlflow.get_run(selected_run_id)
                st.write("##### Th√¥ng tin run")
                st.write(f"*Run ID:* {run.info.run_id}")
                st.write(f"*Experiment ID:* {run.info.experiment_id}")
                st.write(f"*Start Time:* {run.info.start_time}")

                # Hi·ªÉn th·ªã metrics
                st.write("##### Metrics")
                st.json(run.data.metrics)

                # Hi·ªÉn th·ªã params
                st.write("##### Params")
                st.json(run.data.params)

                # Hi·ªÉn th·ªã artifacts s·ª≠ d·ª•ng client.list_artifacts
                artifacts = client.list_artifacts(selected_run_id)
                if artifacts:
                    st.write("##### Artifacts")
                    for artifact in artifacts:
                        st.write(f"- {artifact.path}")
            else:
                st.warning("Kh√¥ng c√≥ runs n√†o trong th√≠ nghi·ªám n√†y.")
        else:
            st.warning("Kh√¥ng c√≥ th√≠ nghi·ªám n√†o ƒë∆∞·ª£c t√¨m th·∫•y.")
    except Exception as e:
        st.error(f"ƒê√£ x·∫£y ra l·ªói khi l·∫•y danh s√°ch th√≠ nghi·ªám: {e}")