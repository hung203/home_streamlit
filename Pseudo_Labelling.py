import datetime
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
import streamlit as st
import mlflow
import mlflow.pytorch
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import random
import cv2
from matplotlib import patches
import pandas as pd
from PIL import Image
from streamlit_drawable_canvas import st_canvas

# Cache d·ªØ li·ªáu MNIST
@st.cache_data
def load_mnist(sample_size):
    mnist = fetch_openml('mnist_784', version=1, as_frame=False)
    X, y = mnist.data / 255.0, mnist.target.astype(int)
    if sample_size < mnist.data.shape[0]:
        X, _, y, _ = train_test_split(X, y, train_size=sample_size / mnist.data.shape[0], random_state=42, stratify=y)
    return X, y

# Cache m√¥ h√¨nh Neural Network
@st.cache_resource
def create_model(num_hidden_layers, hidden_size, activation):
    class SimpleNN(nn.Module):
        def __init__(self):
            super(SimpleNN, self).__init__()
            layers = [nn.Linear(784, hidden_size)]
            if activation == "ReLU":
                layers.append(nn.ReLU())
            elif activation == "Sigmoid":
                layers.append(nn.Sigmoid())
            elif activation == "Tanh":
                layers.append(nn.Tanh())
            for _ in range(num_hidden_layers - 1):
                layers.append(nn.Linear(hidden_size, hidden_size))
                if activation == "ReLU":
                    layers.append(nn.ReLU())
                elif activation == "Sigmoid":
                    layers.append(nn.Sigmoid())
                elif activation == "Tanh":
                    layers.append(nn.Tanh())
            layers.append(nn.Linear(hidden_size, 10))
            self.network = nn.Sequential(*layers)

        def forward(self, x):
            return self.network(x)
    return SimpleNN()

# Ti√™u ƒë·ªÅ ·ª©ng d·ª•ng
st.title("Ph√¢n lo·∫°i ch·ªØ s·ªë vi·∫øt tay MNIST v·ªõi Self-Training Neural Network")

# T·∫°o c√°c tab
tab1, tab2, tab3, tab4 = st.tabs(["L√Ω thuy·∫øt", "Hu·∫•n luy·ªán", "D·ª± ƒêo√°n", "MLflow"])

# Tab 1: L√Ω thuy·∫øt
with tab1:
    st.title(":brain: Hi·ªÉu Bi·∫øt v·ªÅ Pseudo-Labeling trong H·ªçc B√°n Gi√°m S√°t")

    st.header(":book: 1. Pseudo-Labeling l√† g√¨?")
    st.write("""
    :information_source: Pseudo-Labeling l√† m·ªôt k·ªπ thu·∫≠t h·ªçc b√°n gi√°m s√°t nh·∫±m t·∫≠n d·ª•ng d·ªØ li·ªáu kh√¥ng nh√£n (unlabeled data) b·∫±ng c√°ch:
    - S·ª≠ d·ª•ng m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu c√≥ nh√£n ƒë·ªÉ d·ª± ƒëo√°n nh√£n cho d·ªØ li·ªáu kh√¥ng nh√£n.
    - Ch·ªçn c√°c nh√£n d·ª± ƒëo√°n (pseudo-labels) ƒë·ªß t·ª± tin (d·ª±a tr√™n ng∆∞·ª°ng) ƒë·ªÉ th√™m v√†o t·∫≠p d·ªØ li·ªáu c√≥ nh√£n.
    - Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh tr√™n t·∫≠p d·ªØ li·ªáu m·ªü r·ªông.
    """)

    st.header(":question: 2. T·∫°i sao c·∫ßn Pseudo-Labeling?")
    st.write("""
    :star: **D·ªØ li·ªáu c√≥ nh√£n √≠t**: Thu th·∫≠p nh√£n t·ªën k√©m, trong khi d·ªØ li·ªáu kh√¥ng nh√£n th∆∞·ªùng d·ªìi d√†o.  
    :star: **C·∫£i thi·ªán hi·ªáu su·∫•t**: T·∫≠n d·ª•ng d·ªØ li·ªáu kh√¥ng nh√£n ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh.  
    :star: **·ª®ng d·ª•ng th·ª±c t·∫ø**: V√≠ d·ª•: ph√¢n lo·∫°i ·∫£nh (nh∆∞ MNIST) khi ch·ªâ c√≥ m·ªôt ph·∫ßn nh·ªè d·ªØ li·ªáu ƒë∆∞·ª£c g·∫Øn nh√£n.
    """)

    st.header(":gear: 3. Quy tr√¨nh Pseudo-Labeling trong Self-Training")
    st.write(":memo: D∆∞·ªõi ƒë√¢y l√† c√°c b∆∞·ªõc c∆° b·∫£n c·ªßa Pseudo-Labeling v·ªõi c√¥ng th·ª©c minh h·ªça:")

    st.subheader("B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu")
    st.write("T·∫≠p Labeled (L): D·ªØ li·ªáu c√≥ nh√£n ban ƒë·∫ßu:")
    st.latex(r"L = \{(x_i, y_i)\}_{i=1}^{N_L}")
    st.write("T·∫≠p Unlabeled (U): D·ªØ li·ªáu kh√¥ng nh√£n:")
    st.latex(r"U = \{x_j\}_{j=1}^{N_U}")
    
    st.subheader("B∆∞·ªõc 2: Hu·∫•n luy·ªán m√¥ h√¨nh ban ƒë·∫ßu")
    st.write("D√πng t·∫≠p \( L \) ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh \( f(x; \theta) \):")
    st.latex(r"\min_{\theta} \sum_{(x_i, y_i) \in L} \text{Loss}(f(x_i; \theta), y_i)")
    
    st.subheader("B∆∞·ªõc 3: D·ª± ƒëo√°n nh√£n gi·∫£")
    st.write("D·ª± ƒëo√°n tr√™n t·∫≠p \( U \) b·∫±ng \( f(x; \theta) \):")
    st.latex(r"y_{pseudo,j} = \arg\max_{k} (p_j(k))")
    
    st.subheader("B∆∞·ªõc 4: L·ªçc b·∫±ng ng∆∞·ª°ng")
    st.write("Ch·ªçn m·∫´u n·∫øu x√°c su·∫•t t·ªëi ƒëa v∆∞·ª£t ng∆∞·ª°ng \( \tau \):")
    st.latex(r"\max_{k} (p_j(k)) \geq \tau")
    
    st.subheader("B∆∞·ªõc 5: C·∫≠p nh·∫≠t t·∫≠p d·ªØ li·ªáu")
    st.write("Th√™m m·∫´u ƒë∆∞·ª£c ch·ªçn v√†o \( L \):")
    st.latex(r"L = L \cup \{(x_j, y_{pseudo,j})\}")
    st.write("Lo·∫°i m·∫´u kh·ªèi \( U \):")
    st.latex(r"U = U \setminus \{x_j\}")
    
    st.subheader("B∆∞·ªõc 6: L·∫∑p l·∫°i")
    st.write("- Hu·∫•n luy·ªán l·∫°i \( f(x; \theta) \) tr√™n \( L \) m·ªõi.")
    st.write("- L·∫∑p l·∫°i t·ª´ B∆∞·ªõc 3 cho ƒë·∫øn khi \( U = \emptyset \) ho·∫∑c ƒë·∫°t s·ªë v√≤ng l·∫∑p t·ªëi ƒëa.")
    
    st.header("4. ∆Øu ƒëi·ªÉm v√† H·∫°n ch·∫ø")
    st.subheader(":thumbsup: ∆Øu ƒëi·ªÉm:")
    st.write("""
    - :zap: ƒê∆°n gi·∫£n, d·ªÖ tri·ªÉn khai.  
    - :rocket: T·∫≠n d·ª•ng d·ªØ li·ªáu kh√¥ng nh√£n hi·ªáu qu·∫£.  
    - :chart_with_upwards_trend: C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c khi d·ªØ li·ªáu c√≥ nh√£n √≠t.
    """)
    
    st.subheader(":thumbsdown: H·∫°n ch·∫ø:")
    st.write("""
    - :warning: Nh·∫°y c·∫£m v·ªõi nhi·ªÖu: Nh√£n gi·∫£ sai c√≥ th·ªÉ l√†m gi·∫£m ch·∫•t l∆∞·ª£ng m√¥ h√¨nh.  
    - :scales: Ph·ª• thu·ªôc ng∆∞·ª°ng: Ng∆∞·ª°ng cao ‚Üí √≠t nh√£n gi·∫£, ng∆∞·ª°ng th·∫•p ‚Üí nhi·ªÅu nh√£n sai.  
    - :muscle: Y√™u c·∫ßu m√¥ h√¨nh ban ƒë·∫ßu t·ªët ƒë·ªÉ d·ª± ƒëo√°n ch√≠nh x√°c.
    """)
    
    st.header(":tada: 5. K·∫øt lu·∫≠n")
    st.write("""
    :light_bulb: Pseudo-Labeling l√† m·ªôt k·ªπ thu·∫≠t m·∫°nh m·∫Ω trong h·ªçc b√°n gi√°m s√°t, ƒë·∫∑c bi·ªát khi b·∫°n c√≥ √≠t d·ªØ li·ªáu c√≥ nh√£n. Hi·ªáu qu·∫£ c·ªßa n√≥ ph·ª• thu·ªôc v√†o ng∆∞·ª°ng \( \tau \), ch·∫•t l∆∞·ª£ng m√¥ h√¨nh ban ƒë·∫ßu \( f(x; \theta) \), v√† c√°ch c·∫•u h√¨nh qu√° tr√¨nh l·∫∑p.
    """)

# Tab 2: Hu·∫•n luy·ªán
with tab2:
    st.header("1. Ch·ªçn k√≠ch th∆∞·ªõc v√† chia t·∫≠p d·ªØ li·ªáu")

    # Kh·ªüi t·∫°o tr·∫°ng th√°i d·ªØ li·ªáu
    if "mnist_loaded" not in st.session_state:
        mnist = fetch_openml('mnist_784', version=1, as_frame=False)
        st.session_state.total_samples = mnist.data.shape[0]
        st.session_state.mnist_data = mnist
        st.session_state.mnist_loaded = False
        st.session_state.data_split_done = False

    sample_size = st.number_input(
        "Ch·ªçn s·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu",
        min_value=1000,
        max_value=st.session_state.total_samples,
        value=10000,
        step=1000,
        help="S·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu ƒë∆∞·ª£c l·∫•y t·ª´ MNIST (t·ªëi ƒëa 70,000)."
    )

    test_size = st.slider(
        "Ch·ªçn t·ª∑ l·ªá d·ªØ li·ªáu Test",
        0.1, 0.5, 0.2, 0.05,
        help="T·ª∑ l·ªá d·ªØ li·ªáu d√πng ƒë·ªÉ ki·ªÉm tra m√¥ h√¨nh (10%-50%)."
    )
    valid_size = st.slider(
        "Ch·ªçn t·ª∑ l·ªá d·ªØ li·ªáu Validation t·ª´ Train",
        0.1, 0.3, 0.2, 0.05,
        help="T·ª∑ l·ªá d·ªØ li·ªáu t·ª´ t·∫≠p Train d√πng ƒë·ªÉ ki·ªÉm tra trong l√∫c hu·∫•n luy·ªán."
    )

    if st.button("Chia t√°ch d·ªØ li·ªáu"):
        mnist = st.session_state.mnist_data
        X, y = mnist.data / 255.0, mnist.target.astype(int)

        if sample_size < st.session_state.total_samples:
            X, _, y, _ = train_test_split(X, y, train_size=sample_size / st.session_state.total_samples, random_state=42, stratify=y)

        st.session_state.X = X
        st.session_state.y = y

        # Chia t·∫≠p Train v√† Test
        X_train_full, X_test, y_train_full, y_test = train_test_split(
            st.session_state.X, st.session_state.y, test_size=test_size, random_state=42, stratify=st.session_state.y
        )

        # Chia t·∫≠p Train th√†nh Train v√† Validation
        X_train, X_valid, y_train, y_valid = train_test_split(
            X_train_full, y_train_full, test_size=valid_size, random_state=42, stratify=y_train_full
        )

        # T·ª´ t·∫≠p Train, l·∫•y 1% m·ªói l·ªõp l√†m t·∫≠p labeled ban ƒë·∫ßu
        X_labeled = []
        y_labeled = []
        X_unlabeled = []
        y_unlabeled = []
        for digit in range(10):
            digit_indices = np.where(y_train == digit)[0]
            num_samples = len(digit_indices)
            num_labeled = max(1, int(num_samples * 0.01))  # L·∫•y 1%, ƒë·∫£m b·∫£o √≠t nh·∫•t 1 m·∫´u
            labeled_indices = np.random.choice(digit_indices, num_labeled, replace=False)
            unlabeled_indices = np.setdiff1d(digit_indices, labeled_indices)

            X_labeled.append(X_train[labeled_indices])
            y_labeled.append(y_train[labeled_indices])
            X_unlabeled.append(X_train[unlabeled_indices])
            y_unlabeled.append(y_train[unlabeled_indices])

        X_labeled = np.concatenate(X_labeled)
        y_labeled = np.concatenate(y_labeled)
        X_unlabeled = np.concatenate(X_unlabeled)
        y_unlabeled = np.concatenate(y_unlabeled)  # Ground truth cho ƒë√°nh gi√°

        # L∆∞u v√†o session_state
        st.session_state.X_train_labeled = X_labeled
        st.session_state.y_train_labeled = y_labeled
        st.session_state.X_train_unlabeled = X_unlabeled
        st.session_state.y_train_unlabeled = y_unlabeled
        st.session_state.X_valid = X_valid
        st.session_state.y_valid = y_valid
        st.session_state.X_test = X_test
        st.session_state.y_test = y_test
        st.session_state.data_split_done = True
        st.session_state.mnist_loaded = True

        st.write(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia t√°ch v·ªõi {sample_size} m·∫´u!")
        st.write(f"- D·ªØ li·ªáu Train t·ªïng: {X_train.shape} ({(1 - test_size) * (1 - valid_size) * 100:.1f}%)")
        st.write(f"  + Train c√≥ nh√£n (1% m·ªói l·ªõp): {X_labeled.shape}")
        st.write(f"  + Train kh√¥ng nh√£n: {X_unlabeled.shape}")
        st.write(f"- D·ªØ li·ªáu Validation: {X_valid.shape} ({(1 - test_size) * valid_size * 100:.1f}%)")
        st.write(f"- D·ªØ li·ªáu Test: {X_test.shape} ({test_size * 100:.1f}%)")

    # C·∫•u h√¨nh hu·∫•n luy·ªán Self-Training
    st.header("2. Hu·∫•n luy·ªán Neural Network v·ªõi Self-Training")
    # Ti√™u ƒë·ªÅ cho ph·∫ßn tham s·ªë Neural Network
    st.subheader("Tham s·ªë m·∫°ng Neural Network")
    num_epochs = st.number_input("S·ªë epochs m·ªói v√≤ng", min_value=1, max_value=50, value=10)
    batch_size = st.selectbox("Batch size", [16, 32, 64, 128, 256], index=1)
    learning_rate = st.number_input("T·ªëc ƒë·ªô h·ªçc", min_value=0.0001, max_value=0.1, value=0.001, step=0.0001)
    num_hidden_layers = st.number_input("S·ªë l·ªõp ·∫©n", min_value=1, max_value=100, value=1)
    hidden_neurons = st.selectbox("S·ªë n∆°-ron m·ªói l·ªõp ·∫©n", [16, 32, 64, 128, 256], index=1)
    activation_function = st.selectbox("H√†m k√≠ch ho·∫°t", ["ReLU", "Sigmoid", "Tanh"], index=0)

    # Ti√™u ƒë·ªÅ cho ph·∫ßn tham s·ªë Pseudo Labeling
    st.subheader("Tham s·ªë g√°n nh√£n gi·∫£ (Pseudo Labeling)")
    threshold = st.slider("Ng∆∞·ª°ng g√°n Pseudo Label", 0.5, 0.99, 0.95, 0.01)
    max_iterations = st.number_input("S·ªë v√≤ng l·∫∑p t·ªëi ƒëa", min_value=1, max_value=20, value=5)

    experiment_name = st.text_input(
        "Nh·∫≠p t√™n cho th√≠ nghi·ªám MLflow",
        value=f"Self_Training_MNIST_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}"
    )

    if st.button("B·∫Øt ƒë·∫ßu Self-Training"):
        if not st.session_state.get("data_split_done", False):
            st.error("Vui l√≤ng chia t√°ch d·ªØ li·ªáu tr∆∞·ªõc!")
        else:
            X_labeled = st.session_state.X_train_labeled
            y_labeled = st.session_state.y_train_labeled
            X_unlabeled = st.session_state.X_train_unlabeled
            y_unlabeled = st.session_state.y_train_unlabeled  # Ground truth
            X_valid = st.session_state.X_valid
            y_valid = st.session_state.y_valid
            X_test = st.session_state.X_test
            y_test = st.session_state.y_test

            # ƒê·ªãnh nghƒ©a m√¥ h√¨nh Neural Network
            class SimpleNN(nn.Module):
                def __init__(self, num_hidden_layers, hidden_size, activation):
                    super(SimpleNN, self).__init__()
                    layers = [nn.Linear(784, hidden_size)]
                    if activation == "ReLU":
                        layers.append(nn.ReLU())
                    elif activation == "Sigmoid":
                        layers.append(nn.Sigmoid())
                    elif activation == "Tanh":
                        layers.append(nn.Tanh())
                    for _ in range(num_hidden_layers - 1):
                        layers.append(nn.Linear(hidden_size, hidden_size))
                        if activation == "ReLU":
                            layers.append(nn.ReLU())
                        elif activation == "Sigmoid":
                            layers.append(nn.Sigmoid())
                        elif activation == "Tanh":
                            layers.append(nn.Tanh())
                    layers.append(nn.Linear(hidden_size, 10))
                    self.network = nn.Sequential(*layers)

                def forward(self, x):
                    return self.network(x)

            # Thi·∫øt l·∫≠p MLflow
            mlflow.set_experiment(experiment_name)
            with mlflow.start_run() as run:
                # Log c√°c tham s·ªë
                mlflow.log_param("num_epochs", num_epochs)
                mlflow.log_param("batch_size", batch_size)
                mlflow.log_param("learning_rate", learning_rate)
                mlflow.log_param("num_hidden_layers", num_hidden_layers)
                mlflow.log_param("hidden_neurons", hidden_neurons)
                mlflow.log_param("activation_function", activation_function)
                mlflow.log_param("threshold", threshold)
                mlflow.log_param("max_iterations", max_iterations)
                mlflow.log_param("test_size", test_size)
                mlflow.log_param("valid_size", valid_size)
                mlflow.log_param("sample_size", sample_size)

                progress_bar = st.progress(0)
                status_text = st.empty()
                test_acc_history = []
                valid_acc_history = []

                # V√≤ng l·∫∑p Self-Training
                for iteration in range(max_iterations):
                    # (2) Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n t·∫≠p labeled
                    model = SimpleNN(num_hidden_layers, hidden_neurons, activation_function)
                    criterion = nn.CrossEntropyLoss()
                    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

                    X_labeled_tensor = torch.tensor(X_labeled, dtype=torch.float32)
                    y_labeled_tensor = torch.tensor(y_labeled, dtype=torch.long)
                    train_dataset = TensorDataset(X_labeled_tensor, y_labeled_tensor)
                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

                    model.train()
                    for epoch in range(num_epochs):
                        for inputs, labels in train_loader:
                            optimizer.zero_grad()
                            outputs = model(inputs)
                            loss = criterion(outputs, labels)
                            loss.backward()
                            optimizer.step()

                    # (3) D·ª± ƒëo√°n nh√£n cho t·∫≠p unlabeled
                    model.eval()
                    X_unlabeled_tensor = torch.tensor(X_unlabeled, dtype=torch.float32)
                    with torch.no_grad():
                        outputs = model(X_unlabeled_tensor)
                        probabilities = torch.softmax(outputs, dim=1).numpy()
                        predictions = np.argmax(probabilities, axis=1)
                        max_probs = np.max(probabilities, axis=1)

                    # (4) G√°n Pseudo Label v·ªõi ng∆∞·ª°ng
                    pseudo_mask = max_probs >= threshold
                    X_pseudo = X_unlabeled[pseudo_mask]
                    y_pseudo = predictions[pseudo_mask]

                    # (5) C·∫≠p nh·∫≠t t·∫≠p labeled
                    X_labeled = np.concatenate([X_labeled, X_pseudo])
                    y_labeled = np.concatenate([y_labeled, y_pseudo])
                    X_unlabeled = X_unlabeled[~pseudo_mask]

                    # ƒê√°nh gi√° tr√™n t·∫≠p Validation
                    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)
                    y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)
                    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)
                    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)
                    correct = 0
                    total = 0
                    with torch.no_grad():
                        for inputs, labels in valid_loader:
                            outputs = model(inputs)
                            _, predicted = torch.max(outputs.data, 1)
                            total += labels.size(0)
                            correct += (predicted == labels).sum().item()
                    valid_acc = correct / total
                    valid_acc_history.append(valid_acc)

                    # ƒê√°nh gi√° tr√™n t·∫≠p Test
                    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
                    y_test_tensor = torch.tensor(y_test, dtype=torch.long)
                    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
                    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                    correct = 0
                    total = 0
                    with torch.no_grad():
                        for inputs, labels in test_loader:
                            outputs = model(inputs)
                            _, predicted = torch.max(outputs.data, 1)
                            total += labels.size(0)
                            correct += (predicted == labels).sum().item()
                    test_acc = correct / total
                    test_acc_history.append(test_acc)

                    # Log k·∫øt qu·∫£
                    mlflow.log_metric("labeled_size", len(X_labeled), step=iteration)
                    mlflow.log_metric("unlabeled_size", len(X_unlabeled), step=iteration)
                    mlflow.log_metric("valid_accuracy", valid_acc, step=iteration)
                    mlflow.log_metric("test_accuracy", test_acc, step=iteration)

                    progress = (iteration + 1) / max_iterations
                    progress_bar.progress(progress)
                    status_text.text(f"Iteration {iteration+1}/{max_iterations}, Labeled: {len(X_labeled)}, Valid Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}")

                    # D·ª´ng n·∫øu kh√¥ng c√≤n d·ªØ li·ªáu unlabeled
                    if len(X_unlabeled) == 0:
                        st.write("ƒê√£ g√°n nh√£n h·∫øt d·ªØ li·ªáu kh√¥ng nh√£n!")
                        break

                # L∆∞u m√¥ h√¨nh
                mlflow.pytorch.log_model(model, "model")
                st.session_state.model = model
                st.session_state.run_id = run.info.run_id

                st.success("Qu√° tr√¨nh Self-Training ho√†n t·∫•t!")
                st.write("### K·∫øt qu·∫£ cu·ªëi c√πng:")
                st.write(f"- **ƒê·ªô ch√≠nh x√°c tr√™n Validation**: {valid_acc_history[-1]:.4f}")
                st.write(f"- **ƒê·ªô ch√≠nh x√°c tr√™n Test**: {test_acc_history[-1]:.4f}")

                # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ti·∫øn tr√¨nh
                st.subheader("Ti·∫øn tr√¨nh Self-Training")
                fig, ax = plt.subplots()
                ax.plot(range(1, len(test_acc_history) + 1), test_acc_history, label="Test Accuracy")
                ax.plot(range(1, len(valid_acc_history) + 1), valid_acc_history, label="Validation Accuracy")
                ax.set_xlabel("Iteration")
                ax.set_ylabel("Accuracy")
                ax.legend()
                st.pyplot(fig)

                # Hi·ªÉn th·ªã 10 m·∫´u v√≠ d·ª• t·ª´ t·∫≠p Test sau khi hu·∫•n luy·ªán
                st.subheader("10 m·∫´u v√≠ d·ª• t·ª´ t·∫≠p Test v·ªõi d·ª± ƒëo√°n")
                num_examples = 10  # S·ªë l∆∞·ª£ng m·∫´u mu·ªën hi·ªÉn th·ªã
                random_indices = np.random.choice(len(X_test), num_examples, replace=False)
                X_samples = X_test[random_indices]
                y_true = y_test[random_indices]

                # D·ª± ƒëo√°n nh√£n b·∫±ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
                model.eval()
                X_samples_tensor = torch.tensor(X_samples, dtype=torch.float32)
                with torch.no_grad():
                    outputs = model(X_samples_tensor)
                    y_pred = torch.argmax(outputs, dim=1).numpy()

                # T·∫°o figure ƒë·ªÉ hi·ªÉn th·ªã c√°c m·∫´u (2 h√†ng, m·ªói h√†ng 5 m·∫´u)
                fig, axes = plt.subplots(2, 5, figsize=(10, 4))
                for i, (sample, true_label, pred_label) in enumerate(zip(X_samples, y_true, y_pred)):
                    row = i // 5
                    col = i % 5
                    image = sample.reshape(28, 28)
                    axes[row, col].imshow(image, cmap='gray')
                    axes[row, col].set_title(f"Th·ª±c: {true_label}\nD·ª± ƒëo√°n: {pred_label}")
                    axes[row, col].axis('off')
                plt.tight_layout()
                st.pyplot(fig)

    if st.button("B·∫Øt ƒë·∫ßu Self-Training"):
        run_self_training()

# Tab 3: D·ª± ƒëo√°n
with tab3:
    def preprocess_uploaded_image(image):
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        image = cv2.resize(image, (28, 28))
        image = image.reshape(1, -1) / 255.0
        return image

    def preprocess_canvas_image(image_data):
        image = np.array(image_data)[:, :, 0]
        image = cv2.resize(image, (28, 28))
        image = image.reshape(1, -1) / 255.0
        return image

    if "model" not in st.session_state:
        st.error("‚ö†Ô∏è M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán! H√£y quay l·∫°i tab 'Hu·∫•n luy·ªán' ƒë·ªÉ hu·∫•n luy·ªán tr∆∞·ªõc.")
        st.stop()

    st.header("üñçÔ∏è D·ª± ƒëo√°n ch·ªØ s·ªë vi·∫øt tay")
    option = st.radio("üñºÔ∏è Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p:", ["üìÇ T·∫£i ·∫£nh l√™n", "‚úèÔ∏è V·∫Ω s·ªë"])

    # Fragment cho d·ª± ƒëo√°n
    @st.fragment
    def predict_image():
        if option == "üìÇ T·∫£i ·∫£nh l√™n":
            uploaded_file = st.file_uploader("üì§ T·∫£i ·∫£nh s·ªë vi·∫øt tay (PNG, JPG)", type=["png", "jpg", "jpeg"])
            if uploaded_file is not None:
                image = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), cv2.IMREAD_COLOR)
                processed_image = preprocess_uploaded_image(image)
                st.image(image, caption="üì∑ ·∫¢nh t·∫£i l√™n", use_column_width=True)

                if st.button("üîÆ D·ª± ƒëo√°n"):
                    model = st.session_state.model
                    model.eval()
                    with torch.no_grad():
                        input_tensor = torch.tensor(processed_image, dtype=torch.float32)
                        outputs = model(input_tensor)
                        probabilities = torch.softmax(outputs, dim=1).numpy()[0]
                        prediction = np.argmax(probabilities)
                        st.write(f"üéØ **D·ª± ƒëo√°n: {prediction}**")
                        st.write(f"üî¢ **ƒê·ªô tin c·∫≠y: {probabilities[prediction] * 100:.2f}%**")

        elif option == "‚úèÔ∏è V·∫Ω s·ªë":
            canvas_result = st_canvas(
                fill_color="rgba(255, 255, 255, 0.0)",
                stroke_width=15,
                stroke_color="white",
                background_color="black",
                width=280,
                height=280,
                drawing_mode="freedraw",
                key="canvas"
            )
            if st.button("üîÆ D·ª± ƒëo√°n"):
                if canvas_result.image_data is not None:
                    processed_canvas = preprocess_canvas_image(canvas_result.image_data)
                    model = st.session_state.model
                    model.eval()
                    with torch.no_grad():
                        input_tensor = torch.tensor(processed_canvas, dtype=torch.float32)
                        outputs = model(input_tensor)
                        probabilities = torch.softmax(outputs, dim=1).numpy()[0]
                        prediction = np.argmax(probabilities)
                        st.write(f"üéØ **D·ª± ƒëo√°n: {prediction}**")
                        st.write(f"üî¢ **ƒê·ªô tin c·∫≠y: {probabilities[prediction] * 100:.2f}%**")

    predict_image()

# Tab 4: MLflow
with tab4:
    st.header("Tracking MLflow")
    try:
        from mlflow.tracking import MlflowClient
        client = MlflowClient()

        experiments = mlflow.search_experiments()
        if experiments:
            st.write("#### Danh s√°ch th√≠ nghi·ªám")
            experiment_data = [{"Experiment ID": exp.experiment_id, "Experiment Name": exp.name, "Artifact Location": exp.artifact_location} for exp in experiments]
            df_experiments = pd.DataFrame(experiment_data)
            st.dataframe(df_experiments)

            selected_exp_name = st.selectbox("üîç Ch·ªçn th√≠ nghi·ªám ƒë·ªÉ xem chi ti·∫øt", options=[exp.name for exp in experiments])
            selected_exp_id = next(exp.experiment_id for exp in experiments if exp.name == selected_exp_name)

            runs = mlflow.search_runs(selected_exp_id)
            if not runs.empty:
                st.write("#### Danh s√°ch runs")
                st.dataframe(runs)

                selected_run_id = st.selectbox("üîç Ch·ªçn run ƒë·ªÉ xem chi ti·∫øt", options=runs["run_id"])
                run = mlflow.get_run(selected_run_id)
                st.write("##### Th√¥ng tin run")
                st.write(f"*Run ID:* {run.info.run_id}")
                st.write(f"*Experiment ID:* {run.info.experiment_id}")
                st.write(f"*Start Time:* {run.info.start_time}")

                st.write("##### Metrics")
                st.json(run.data.metrics)

                st.write("##### Params")
                st.json(run.data.params)

                artifacts = client.list_artifacts(selected_run_id)
                if artifacts:
                    st.write("##### Artifacts")
                    for artifact in artifacts:
                        st.write(f"- {artifact.path}")
            else:
                st.warning("Kh√¥ng c√≥ runs n√†o trong th√≠ nghi·ªám n√†y.")
        else:
            st.warning("Kh√¥ng c√≥ th√≠ nghi·ªám n√†o ƒë∆∞·ª£c t√¨m th·∫•y.")
    except Exception as e:
        st.error(f"ƒê√£ x·∫£y ra l·ªói khi l·∫•y danh s√°ch th√≠ nghi·ªám: {e}")
